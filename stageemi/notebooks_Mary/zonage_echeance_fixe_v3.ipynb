{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import xarray as xr \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "import sys, os\n",
    "import string\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('./lib'))\n",
    "\n",
    "from lib import hss,precision #,far,f1, pod,pofd\n",
    "from lib import create_combination_subzones, create_nc_mask_NSEO\n",
    "from lib import find_neighbours,get_optimal_subzone_v2, group_masks_size, select_group_mask, get_WME_legend, get_not_included_masks\n",
    "\n",
    "import stageemi\n",
    "import stageemi.dev.distance_wwmf as distance_wwmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['legend.handlelength'] = 0\n",
    "matplotlib.rcParams['legend.numpoints'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dep_id 38\n",
      "fin calcul distance\n",
      "44\n",
      "[ 2.  4.  8. 10. 14.] ['Nuageux', 'Brume/Brouillard/Grisaille', 'Pluie', 'Neige', 'Averses']\n",
      "{10: ['0+6+1'], 14: ['8'], 8: ['4']}\n",
      "['2', '3', '5', '7']\n"
     ]
    }
   ],
   "source": [
    "# On obtient un zonage par departement par echeance.\n",
    "\n",
    "''' input '''\n",
    "date = '2020012600' # Date pour laquelle on fait tourner \n",
    "# date = '2020030600'\n",
    "list_method_distance = ['compas']#,'agat','compas_asym','agat_asym'] # pour agreger le temps sensible\n",
    "\n",
    "mask_sympo = False # Veut-on des combie de zones sympos ? \n",
    "mask_geographique = True # Veut-on des combinaisons Est/Ouest/Nord/Sud. A rebrancher. \n",
    "# mask_sympo = True # Veut-on des combie de zones sympos ? \n",
    "# mask_geographique = False # Veut-on des combinaisons Est/Ouest/Nord/Sud. A rebrancher. \n",
    "\n",
    "\n",
    "dir_fig = '../figures/total/' \n",
    "nsubzonesMax = 4 # Nombre de sous zones \n",
    "plot_results = False\n",
    "Force = False # Force to recompute staff \n",
    "if date == '2020012600':\n",
    "#     echeance_dict = {\n",
    "#         '38':[44,12,3,46,43,25,30],\n",
    "#         '29':[32,39,20,33,13],  \n",
    "#         '34':[1,5,6,4 ,10, 20,30], \n",
    "#         '41':[45,4,44,5,20,30]\n",
    "#     }\n",
    "        echeance_dict = {\n",
    "            '38':[44]\n",
    "#         '41':[45] #,5]\n",
    "    }\n",
    "elif date == '2020030600':\n",
    "    echeance_dict = {\n",
    "        '38':[29,3,1,4,36],\n",
    "        '41':[18],\n",
    "        '29':[1,5,3],  \n",
    "        '34':[31,6,16,29,30], \n",
    "    }\n",
    "    \n",
    "for dep_id in echeance_dict.keys():\n",
    "    echeance_list = echeance_dict[dep_id]\n",
    "    print('dep_id',dep_id)\n",
    "    ''' lecture fichier arome '''\n",
    "    fname = \"../WWMF/\" + date+'0000__PG0PAROME__'+'WWMF'+'__EURW1S100______GRILLE____0_48_1__SOL____GRIB2.nc'\n",
    "    ds = xr.open_dataset(fname,chunks={\"step\":1}).isel(step = echeance_list)\n",
    "    # Arrondi pour éviter les erreurs     \n",
    "    ds['latitude']  = ds['latitude'].round(5)\n",
    "    ds['longitude'] = ds['longitude'].round(5)\n",
    "    if date == '2020030600':\n",
    "        ds = ds.rename({'paramId_0':'unknown'})\n",
    "    \n",
    "\n",
    "        \n",
    "    ''' lecture du mask '''\n",
    "    if mask_sympo and not mask_geographique: \n",
    "        fname_out = '../GeoData/zones_sympo_multiples/'+dep_id+'_mask_zones_sympos.nc'\n",
    "        if not os.path.exists(fname_out): \n",
    "            # Creation du fichier (netcdf) de combinaison des zones sympos \n",
    "            dir_mask = '/home/mrpa/borderiesm/stageEMI/Codes/StageEMI/Masques_netcdf/ZONE_SYMPO/'\n",
    "            list_subzones = glob.glob(dir_mask + dep_id +'*.nc')\n",
    "            n_subzones = len(list_subzones)  # nombre de zones sympos initiales\n",
    "            lst_subzones = [zone[-7:-3] for zone in list_subzones]\n",
    "            ds_mask = create_combination_subzones(dir_mask,dep_id,lst_subzones,fname_out,degre5=True) \n",
    "            ds_mask = ds_mask.chunk({\"id\":1}) # Rend le calcul parallele possible \n",
    "        else: \n",
    "            # Le fichier est disponible \n",
    "            ds_mask = xr.open_dataset(fname_out,chunks={\"id\":1})\n",
    "#         ds_dep_tot = (ds*ds_mask.mask.sel(id=\"departement\").drop(\"id\"))\n",
    "        id_dep = 'departement'\n",
    "    if mask_geographique and not mask_sympo: \n",
    "        if   dep_id == '38': dep = 'FRK24'\n",
    "        elif dep_id == '41': dep = 'FRB05'\n",
    "        elif dep_id == \"34\": dep = 'FRJ13'\n",
    "        elif dep_id == '29': dep = \"FRH02\"\n",
    "        else: \n",
    "            print('remplir la bonne valeur pour le dep')\n",
    "            sys.exit()\n",
    "        fname_out = '../GeoData/zones_sympo_multiples/'+ dep_id+'_'+dep+'_mask_NSEO.nc'\n",
    "        if not os.path.exists(fname_out):\n",
    "            # Creation du fichier (netcdf) s'il n'existe pas \n",
    "            dir_mask  = '../GeoData/nc_departement/'\n",
    "            dep_file  = dir_mask + dep +'.nc' \n",
    "            print('on cree',fname_out)\n",
    "            ds_mask = create_nc_mask_NSEO(dep_file,fname_out,plot_dep=False)\n",
    "            ds_mask = ds_mask.chunk({\"id\":1})\n",
    "        else:\n",
    "            ds_mask = xr.open_dataset(fname_out,chunks={\"id\":1})\n",
    "#         ds_dep_tot = (ds*ds_mask.mask.sel(id=ds_mask.id[ds_mask.id_geo == 'departement'])).squeeze(\"id\",drop=True)\n",
    "        id_dep = '0+1+2+3+4+5+6+7+8'\n",
    "        \n",
    "    # Arrondi pour éviter les erreurs         \n",
    "    ds_mask[\"latitude\"]  = ds_mask[\"latitude\"].round(5)\n",
    "    ds_mask[\"longitude\"] = ds_mask[\"longitude\"].round(5)\n",
    "    ds_dep_tot = (ds*ds_mask.mask.sel(id=id_dep))\n",
    "   \n",
    "    ''' calcul des temps agrégés '''\n",
    "    ds_distance_dict = {}\n",
    "    for name in list_method_distance:\n",
    "        ds_distance         = distance_wwmf.get_pixel_distance_dept(ds_dep_tot,name) # rajoute les variables wme_arr et w1_arr\n",
    "        ds_distance_chunk   = ds_distance.chunk({\"step\":1}) \n",
    "        # On recupere ici toute les zones. \n",
    "        ds_distance_dict[name] = (ds_distance_chunk * ds_mask.mask).sum(['latitude',\"longitude\"]).compute()\n",
    "    print('fin calcul distance')\n",
    "   \n",
    "    # On part toujours sur l'utilisation des dénominations COMPAS car elles sont moins nombreuses?  \n",
    "    var_name = 'wme_arr'\n",
    "    for icheance,echeance in enumerate(echeance_list): \n",
    "        print(echeance)\n",
    "        fname_out = '../zonageWME/v7_'+dep_id+'_'+date+'_'+str(echeance)+'.csv'\n",
    "\n",
    "        if os.path.exists(fname_out) and not Force:\n",
    "            print(fname_out,'existe')\n",
    "            continue\n",
    "        \n",
    "        tdeb = time.time()\n",
    "        ''' on restreint la liste des WME pour le zonage '''\n",
    "        ds_dep = ds_dep_tot.isel(step = icheance).copy()\n",
    "        # on regroupe 'Très nuageux/Couvert' et 'Nuageux'\n",
    "        ds_dep = ds_dep.where(~((ds_dep[var_name].values == 2) + (ds_dep[var_name].values == 3) ), 2)\n",
    "        # on regroupe ensemble neige (10) et neige faible (7)\n",
    "        ds_dep = ds_dep.where(~((ds_dep[var_name].values == 7) + (ds_dep[var_name].values == 10)), 10)\n",
    "        # on regroupe ensemble pluie (8) et pluie faible (6)\n",
    "        ds_dep = ds_dep.where(~((ds_dep[var_name].values == 8) + (ds_dep[var_name].values == 6)),8)\n",
    "        # on regroupe ensemble qlqs averses (12) et averses (14), et qlqs averses de neige (13)\n",
    "        ds_dep = ds_dep.where(~((ds_dep[var_name].values == 12) + (ds_dep[var_name].values == 13)\n",
    "                                  + (ds_dep[var_name].values == 14 )),14)\n",
    "        # on regroupe ensemble averses Orageuses (16) et Orages  (18)\n",
    "        ds_dep = ds_dep.where(~((ds_dep[var_name].values == 16) + (ds_dep[var_name].values == 18)),18)\n",
    "\n",
    "        file_CodesWWMF = '../utils/CodesWWMF.csv'\n",
    "        cible_list,legend_list = get_WME_legend(file_CodesWWMF, ds_dep) \n",
    "        print(cible_list,legend_list)\n",
    "\n",
    "        ''' zonage '''\n",
    "        listCible    = [10,14,8]#cible_list[::-1] # On considère que l'ordre inverse est l'ordre de criticité maximun.\n",
    "                                        # A bien définir lors de l'utilisation selon les cas.  \n",
    "\n",
    "        legend_cible = [] # pour stocker la légende du code WME\n",
    "        listMasksNew = ds_mask.id.values # on commence avec l'ensemble des masks\n",
    "\n",
    "        # liste de zones sympos initiales (pour checker à la fin si on a une info sur toutes les zones du département)\n",
    "        list_zones_sympos_initiales = [zone for zone in ds_mask.id.values if (('+' not in zone) and (zone!='departement'))]\n",
    "        \n",
    "        nsubzones    = 0\n",
    "        zones_cibles = {}\n",
    "        score_zones_cibles = {}\n",
    "        if len(listCible) == 0 : # si un département a le même temps sensible partout\n",
    "            zones_cibles[listCible[0]] = 'departement'\n",
    "        else: \n",
    "            for icible,cible in enumerate(listCible):\n",
    "                if nsubzones > nsubzonesMax: \n",
    "                    print('nombre de sous-zones trop grand')\n",
    "                    break \n",
    "                if nsubzones >1: \n",
    "                    # pour éviter que departement ne soit selectionné alors que des sous-zones de departement aient déjà été selectionnées.\n",
    "                    listMasksNew = [element for element in listMasksNew if element !=id_dep ]\n",
    "\n",
    "                if len(listMasksNew)>60:\n",
    "                    #  on regroupe les masks selon leur taille pour aller plus vite \n",
    "                    groupe1,groupe2,groupe3,taille1,taille2  = group_masks_size(listMasksNew,ds_mask)\n",
    "                    # on selectionne le groupement de zones qui match l'objet météo\n",
    "                    groupe_mask_select = select_group_mask(ds_dep,cible,groupe1,groupe2,groupe3,taille1,taille2)\n",
    "                else: \n",
    "                    # on considère l'ensemble des masks\n",
    "                    groupe_mask_select = ds_mask.mask.sel(id=listMasksNew)\n",
    "                # on selectionne la zone optimale (selon le hss et la précision)\n",
    "                zones_optimales,score_hss,score_precision=get_optimal_subzone_v2(ds_dep, groupe_mask_select,cible,ds_mask)\n",
    "                if len(zones_optimales)!=0:\n",
    "                    legend_cible.append(legend_list[::-1][icible])\n",
    "                    score_zones_cibles[cible] = score_hss\n",
    "                    zones_cibles[cible] = zones_optimales \n",
    "                    nsubzones +=1 \n",
    "                    # sinon pas de zones selectionnées                                            \n",
    "                    ''' on check que la somme des zones n'est pas déjà égale au departement '''\n",
    "                    if  (nsubzones== 1) and (len(zones_cibles[cible]) == 1) :\n",
    "                        ds_temp  = ds_mask.sel(id=zones_cibles[cible][0]).mask.copy()\n",
    "\n",
    "                    elif (nsubzones== 1) and (len(zones_cibles[cible]) > 1): \n",
    "    #                    ds_temp  = ds_mask.sel(id=zones_cibles[cible]).mask.sum(\"id\") >= 1  \n",
    "                        ds_temp  = ds_mask.sel(id=zones_cibles[cible][0]).mask.copy() \n",
    "                        ds_temp.values[(ds_temp.values == 1) + (ds_mask.sel(id=zones_cibles[cible][1]).mask.values ==1) ] = 1\n",
    "                    else: \n",
    "                        for zone in zones_cibles[cible]:                            \n",
    "                            ds_temp.values[(ds_temp.values == 1) + (ds_mask.sel(id=zone).mask.values ==1) ] = 1\n",
    "\n",
    "                    somme = np.sum((ds_temp.values == 1)&( ds_mask.sel(id=id_dep).mask.values== 1))\n",
    "                    tailleDep = np.sum( ds_mask.sel(id=id_dep).mask.values== 1)\n",
    "                    if somme == tailleDep: \n",
    "                        print('on a atteint la taille du departement')\n",
    "                        break\n",
    "                    # on récupère les zones non-incluses dans la zone sélectionnée\n",
    "                    for zone in zones_cibles[cible]:\n",
    "                        listMasksNew, lst_mask_included = get_not_included_masks(ds_mask.mask.sel(id=zone)\n",
    "                                                        ,listMasksNew,ds_mask,flag_strictly_included=False)\n",
    "            # fin boucle sur cible\n",
    "            ''' on vérifie que toutes les zones du département sont dans les zones selectionnées '''\n",
    "            list_zones_select = sum([zones_cibles[cible] for cible in zones_cibles.keys()],[]) \n",
    "            zones_restantes = []\n",
    "            for zone_sympo in list_zones_sympos_initiales:\n",
    "                n = 0\n",
    "                for zone_select in list_zones_select: \n",
    "                    if zone_sympo in zone_select:\n",
    "                        n+=1\n",
    "                if n == 0 : \n",
    "                    zones_restantes.append(zone_sympo)\n",
    "        \n",
    "        print(zones_cibles)   \n",
    "        print(zones_restantes)\n",
    "        continue\n",
    "        '''save results in csv'''\n",
    "        print('saving results')\n",
    "        \n",
    "        d = { 'zone':sum([zones_cibles[cible] for cible in zones_cibles.keys()],[]), \n",
    "            'cible_wme':sum([[cible]  if len(zones_cibles[cible])==1 else [cible,cible] for cible in zones_cibles.keys()],[]),\n",
    "            'hss' : sum([score_zones_cibles[cible] for cible in zones_cibles.keys()],[])}\n",
    "\n",
    "        if len(zones_restantes)>0:\n",
    "            d['zone'] += zones_restantes\n",
    "            d['hss'] += [np.nan for i in range(len(zones_restantes))]\n",
    "            d['cible_wme'] += [np.nan for i in range(len(zones_restantes))]\n",
    "        for name in list_method_distance:\n",
    "            d[name] =  ds_distance_dict[name].wwmf_2[ds_distance_dict[name].argmin(\"wwmf_2\")].sel(id=d['zone']).isel(step=icheance).values\n",
    "        pd.DataFrame(data=d).to_csv(fname_out)\n",
    "        print('temps %s \\n'%(time.time()-tdeb))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['est' 'centre' 'nord-centre']\n",
      "['nord-ouest' 'sud-ouest' 'sud-centre' 'ouest-centre']\n"
     ]
    }
   ],
   "source": [
    "print(ds_mask.id_geo.sel(id=list_zones_select).values)\n",
    "print(ds_mask.id_geo.sel(id=zones_restantes).values)\n",
    "# cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dep_id 41\n",
      "on cree ../GeoData/zones_sympo_multiples/41_FRB05_mask_NSEO.nc\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'departement'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\preproc\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'departement'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6093f1e66c40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'longitude'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'longitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mds_dep_tot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mds_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"departement\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'2020030600'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mds_dep_tot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_dep_tot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'paramId_0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'unknown'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\preproc\\lib\\site-packages\\xarray\\core\\dataarray.py\u001b[0m in \u001b[0;36msel\u001b[1;34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \"\"\"\n\u001b[1;32m-> 1056\u001b[1;33m         ds = self._to_temp_dataset().sel(\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[0mindexers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\preproc\\lib\\site-packages\\xarray\\core\\dataset.py\u001b[0m in \u001b[0;36msel\u001b[1;34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[0m\n\u001b[0;32m   2054\u001b[0m         \"\"\"\n\u001b[0;32m   2055\u001b[0m         \u001b[0mindexers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meither_dict_or_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m         pos_indexers, new_indexes = remap_label_indexers(\n\u001b[0m\u001b[0;32m   2057\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\preproc\\lib\\site-packages\\xarray\\core\\coordinates.py\u001b[0m in \u001b[0;36mremap_label_indexers\u001b[1;34m(obj, indexers, method, tolerance, **indexers_kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m     }\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m     pos_indexers, new_indexes = indexing.remap_label_indexers(\n\u001b[0m\u001b[0;32m    392\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\preproc\\lib\\site-packages\\xarray\\core\\indexing.py\u001b[0m in \u001b[0;36mremap_label_indexers\u001b[1;34m(data_obj, indexers, method, tolerance)\u001b[0m\n\u001b[0;32m    268\u001b[0m             \u001b[0mcoords_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_cast_to_coords_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m             \u001b[0midxr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_label_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m             \u001b[0mpos_indexers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midxr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnew_idx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\preproc\\lib\\site-packages\\xarray\\core\\indexing.py\u001b[0m in \u001b[0;36mconvert_label_indexer\u001b[1;34m(index, label, index_name, method, tolerance)\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m                 indexer = index.get_loc(\n\u001b[0m\u001b[0;32m    190\u001b[0m                     \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\preproc\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'departement'"
     ]
    }
   ],
   "source": [
    "sys.exit()\n",
    "''' input '''\n",
    "date = '2020012600'\n",
    "# date = '2020030600'\n",
    "list_name = ['compas'] #,'agat','compas_asym','agat_asym'] # pour agreger le temps sensible\n",
    "\n",
    "mask_sympo = False\n",
    "mask_geographique = True\n",
    "dir_fig = '../figures/total/'\n",
    "nsubzonesMax = 7\n",
    "plot_results = True\n",
    "if date == '2020012600':\n",
    "#     echeance_dict = {\n",
    "#         '38':[44,12,3,46,43,25,30],\n",
    "#         '29':[32,39,20,33,13],  \n",
    "#         '34':[1,5,6,4 ,10, 20,30], \n",
    "#         '41':[45,4,44,5,20,30]\n",
    "#     }\n",
    "        echeance_dict = {\n",
    "        '41':[30]\n",
    "    }\n",
    "if date == '2020030600':\n",
    "    echeance_dict = {\n",
    "        '38':[29,3,1,4,36],\n",
    "        '41':[18],\n",
    "        '29':[1,5,3],  \n",
    "        '34':[31,6,16,29,30], \n",
    "    }\n",
    "    \n",
    "for dep_id in echeance_dict.keys():\n",
    "    echeance_list = echeance_dict[dep_id]\n",
    "    print('dep_id',dep_id)\n",
    "    ''' lecture du mask '''\n",
    "    if mask_sympo and not mask_geographique: \n",
    "        fname_out = '../GeoData/zones_sympo_multiples/'+dep_id+'_mask_zones_sympos.nc'\n",
    "        if not os.path.exists(fname_out): \n",
    "            dir_mask = '/home/mrpa/borderiesm/stageEMI/Codes/StageEMI/Masques_netcdf/ZONE_SYMPO/'\n",
    "            list_subzones = glob.glob(dir_mask + dep_id +'*.nc')\n",
    "            n_subzones = len(list_subzones)  # nombre de zones sympos initiales\n",
    "            lst_subzones = [zone[-7:-3] for zone in list_subzones]\n",
    "            ds_mask = create_combination_subzones(dir_mask,dep_id,lst_subzones,fname_out,degre5=True) \n",
    "            ds_mask = ds_mask.chunk({\"id\":1})\n",
    "        else: \n",
    "            ds_mask = xr.open_dataset(fname_out,chunks={\"id\":1})\n",
    "\n",
    "    if mask_geographique and not mask_sympo: \n",
    "        if   dep_id == '38': dep = 'FRK24'\n",
    "        elif dep_id == '41': dep = 'FRB05'\n",
    "        elif dep_id == \"34\": dep = 'FRJ13'\n",
    "        elif dep_id == '29': dep = \"FRH02\"\n",
    "        else: \n",
    "            print('remplir la bonne valeur pour le dep')\n",
    "            sys.exit()\n",
    "        fname_out = '../GeoData/zones_sympo_multiples/'+ dep_id+'_'+dep+'_mask_NSEO.nc'\n",
    "        if not os.path.exists(fname_out):\n",
    "            dir_mask  = '../GeoData/nc_departement/'\n",
    "            dep_file  = dir_mask + dep +'.nc' \n",
    "            print('on cree',fname_out)\n",
    "            ds_mask = create_nc_mask_NSEO(dep_file,fname_out,plot_dep=False)\n",
    "            ds_mask = ds_mask.chunk({\"id\":1})\n",
    "        else:\n",
    "            ds_mask = xr.open_dataset(fname_out,chunks={\"id\":1})\n",
    "            \n",
    "    ds_mask[\"latitude\"]  = ds_mask[\"latitude\"].round(5)\n",
    "    ds_mask[\"longitude\"] = ds_mask[\"longitude\"].round(5)\n",
    "   \n",
    "    ''' lecture arome '''\n",
    "    fname = \"../WWMF/\" + date+'0000__PG0PAROME__'+'WWMF'+'__EURW1S100______GRILLE____0_48_1__SOL____GRIB2.nc'\n",
    "\n",
    "    ds = xr.open_dataset(fname,chunks={\"step\":1}).isel(step = echeance_list)\n",
    "    ds['latitude']  = ds['latitude'].round(5)\n",
    "    ds['longitude'] = ds['longitude'].round(5)\n",
    "    \n",
    "    ds_dep_tot = (ds*ds_mask.mask.sel(id=\"departement\").drop(\"id\"))\n",
    "#     ds_mask.sel(id= ds_mask.id[ds_mask.id_geo == 'departement'])\n",
    "    if date == '2020030600':\n",
    "        ds_dep_tot = ds_dep_tot.rename({'paramId_0':'unknown'})\n",
    "        \n",
    "    ''' calcul des temps agrégés '''\n",
    "    ds_distance_dict = {}\n",
    "    for name in list_name:\n",
    "        ds_distance         = distance_wwmf.get_pixel_distance_dept(ds_dep_tot,name)\n",
    "        ds_distance_chunk   = ds_distance.chunk({\"step\":1}) \n",
    "        ds_distance_dict[name] = (ds_distance_chunk * ds_mask.mask).sum(['latitude',\"longitude\"]).compute()\n",
    "    print('fin calcul distance')\n",
    "    \n",
    "    var_name = 'wme_arr'\n",
    "    for icheance,echeance in enumerate(echeance_list): \n",
    "        print(echeance)\n",
    "        fname_out = '../zonageWME/geo'+dep_id+'_'+date+'_'+str(echeance)+'.csv'\n",
    "        if os.path.exists(fname_out):\n",
    "            print(fname_out,'existe')\n",
    "            continue\n",
    "        \n",
    "        tdeb = time.time()\n",
    "        ''' on restreint la liste des WME pour le zonage '''\n",
    "        ds_dep = ds_dep_tot.isel(step = icheance).copy()\n",
    "        # on regroupe 'Très nuageux/Couvert' et 'Nuageux'\n",
    "        ds_dep = ds_dep.where(~((ds_dep[var_name].values == 2) + (ds_dep[var_name].values == 3) ), 2)\n",
    "\n",
    "        # on regroupe ensemble neige (10) et neige faible (7)\n",
    "        ds_dep = ds_dep.where(~((ds_dep[var_name].values == 7) + (ds_dep[var_name].values == 10)), 10)\n",
    "        \n",
    "        # on regroupe ensemble pluie (8) et pluie faible (6)\n",
    "        ds_dep = ds_dep.where(~((ds_dep[var_name].values == 8) + (ds_dep[var_name].values == 6)),8)\n",
    "\n",
    "        # on regroupe ensemble qlqs averses (12) et averses (14), et qlqs averses de neige (13)\n",
    "        ds_dep = ds_dep.where(~((ds_dep[var_name].values == 12) + (ds_dep[var_name].values == 13)\n",
    "                                  + (ds_dep[var_name].values == 14 )),14)\n",
    "\n",
    "        # on regroupe ensemble averses Orageuses (16) et Orages  (18)\n",
    "        ds_dep = ds_dep.where(~((ds_dep[var_name].values == 16) + (ds_dep[var_name].values == 18)),18)\n",
    "\n",
    "        file_CodesWWMF = '../utils/CodesWWMF.csv'\n",
    "        cible_list,legend_list = get_WME_legend(file_CodesWWMF, ds_dep)\n",
    "        print(cible_list,legend_list)\n",
    "\n",
    "        ''' zonage '''\n",
    "        listCible    = cible_list[::-1]\n",
    "        legend_cible = [] # pour stocker la légende du code WME\n",
    "        listMasksNew = ds_mask.id.values # on commence avec l'ensemble des masks\n",
    "\n",
    "        # liste de zones sympos initiales (pour checker à la fin si on a une info sur toutes les zones du département)\n",
    "        list_zones_sympos_initiales = [zone for zone in ds_mask.id.values if '+' not in zone]\n",
    "        print(list_zones_sympos_initiales)\n",
    "        sys.exit()\n",
    "        nsubzones    = 0\n",
    "        zones_cibles = {}\n",
    "        score_zones_cibles = {}\n",
    "        if len(listCible) == 0 : # si un département a le même temps sensible partout\n",
    "            zones_cibles[listCible[0]] = 'departement'\n",
    "        else: \n",
    "            for icible,cible in enumerate(listCible):\n",
    "                if nsubzones > nsubzonesMax: \n",
    "                    print('nombre de sous-zones trop grand')\n",
    "                    break \n",
    "                if nsubzones >1: \n",
    "                    # pour éviter que departement ne soit selectionné alors que des sous-zones de departement aient déjà été selectionnées.\n",
    "                    listMasksNew = [element for element in listMasksNew if element !='departement']\n",
    "\n",
    "                if len(listMasksNew)>60:\n",
    "                    #  on regroupe les masks selon leur taille pour aller plus vite \n",
    "                    groupe1,groupe2,groupe3,taille1,taille2  = group_masks_size(listMasksNew,ds_mask)\n",
    "                    # on selectionne le groupement de zones qui match l'objet météo\n",
    "                    groupe_mask_select = select_group_mask(ds_dep,cible,groupe1,groupe2,groupe3,taille1,taille2)\n",
    "                else: \n",
    "                    # on considère l'ensemble des masks\n",
    "                    groupe_mask_select = ds_mask.mask.sel(id=listMasksNew)\n",
    "                # on selectionne la zone optimale (selon le hss et la précision)\n",
    "                zones_optimales,score_hss,score_precision=get_optimal_subzone_v2(ds_dep, groupe_mask_select,cible,ds_mask)\n",
    "                if len(zones_optimales)==0:\n",
    "                    # pas de zone sélectionnée pour ce temps sensible\n",
    "                    continue\n",
    "                else: \n",
    "                    legend_cible.append(legend_list[::-1][icible])\n",
    "                    score_zones_cibles[cible] = score_hss\n",
    "                    zones_cibles[cible] = zones_optimales \n",
    "                    nsubzones +=1                          \n",
    "\n",
    "                ''' on check que la somme des zones n'est pas déjà égale au departement '''\n",
    "                if  (nsubzones== 1) and (len(zones_cibles[cible]) == 1) :\n",
    "                    ds_temp  = ds_mask.sel(id=zones_cibles[cible][0]).mask.copy()\n",
    "\n",
    "                elif (nsubzones== 1) and (len(zones_cibles[cible]) > 1): \n",
    "                    ds_temp  = ds_mask.sel(id=zones_cibles[cible][0]).mask.copy() \n",
    "                    ds_temp.values[(ds_temp.values == 1) + (ds_mask.sel(id=zones_cibles[cible][1]).mask.values ==1) ] = 1\n",
    "                else: \n",
    "                    for zone in zones_cibles[cible]:\n",
    "                        print(zone)\n",
    "                        ds_temp.values[(ds_temp.values == 1) + (ds_mask.sel(id=zone).mask.values ==1) ] = 1\n",
    "\n",
    "                somme = np.sum((ds_temp.values == 1)&( ds_mask.sel(id='departement').mask.values== 1))\n",
    "                tailleDep = np.sum( ds_mask.sel(id='departement').mask.values== 1)\n",
    "                if somme == tailleDep: \n",
    "                    print('on a atteint la taille du departement')\n",
    "                    break\n",
    "                # on récupère les zones non-incluses dans la zone sélectionnée\n",
    "                for zone in zones_cibles[cible]:\n",
    "                    listMasksNew, lst_mask_included = get_not_included_masks(ds_mask.mask.sel(id=zone)\n",
    "                                                    ,listMasksNew,ds_mask,flag_strictly_included=False)\n",
    "            # fin boucle sur cible\n",
    "            ''' on vérifie que toutes les zones du département sont dans les zones selectionnées '''\n",
    "            list_zones_select = sum([zones_cibles[cible] for cible in zones_cibles.keys()],[]) \n",
    "            zones_restantes = []\n",
    "            for zone_sympo in list_zones_sympos_initiales:\n",
    "                n = 0\n",
    "                for zone_select in list_zones_select: \n",
    "                    if zone_sympo in zone_select:\n",
    "                        n+=1\n",
    "                if n == 0 : \n",
    "                    zones_restantes.append(zone_sympo)\n",
    "        \n",
    "        print(zones_cibles)          \n",
    "        '''save results in csv'''\n",
    "        print('saving results')\n",
    "        \n",
    "        d = { 'zone':sum([zones_cibles[cible] for cible in zones_cibles.keys()],[]), \n",
    "            'cible_wme':sum([[cible]  if len(zones_cibles[cible])==1 else [cible,cible] for cible in zones_cibles.keys()],[]),\n",
    "            'hss' : sum([score_zones_cibles[cible] for cible in zones_cibles.keys()],[])}\n",
    "\n",
    "        if len(zones_restantes)>0:\n",
    "            d['zone'] += zones_restantes\n",
    "            d['hss'] += [np.nan for i in range(len(zones_restantes))]\n",
    "            d['cible_wme'] += [np.nan for i in range(len(zones_restantes))]\n",
    "        for name in list_name:\n",
    "            d[name] =  ds_distance_dict[name].wwmf_2[ds_distance_dict[name].argmin(\"wwmf_2\")].sel(id=d['zone']).isel(step=icheance).values\n",
    "        pd.DataFrame(data=d).to_csv(fname_out)\n",
    "        \n",
    "        ''' plot '''\n",
    "        if not plot_results: \n",
    "            continue\n",
    "        print('plot')\n",
    "        X,Y = np.meshgrid( ds_mask.longitude.values,ds_mask.latitude.values)\n",
    "        listMasks = [ds_mask.sel(id=id_ref) for id_ref in list_zones_sympos_initiales]\n",
    "\n",
    "        legende = string.ascii_lowercase\n",
    "        patches = []\n",
    "        fig,axes = plt.subplots(nrows=1,ncols =3,figsize  = (15,5))\n",
    "        ax = axes.flat\n",
    "\n",
    "        fig.subplots_adjust(wspace=0.3)\n",
    "        var2plot_lst = ['unknown','wme_arr','w1_arr']\n",
    "        varmin_lst   = [0,1,0]\n",
    "        varmax_lst   = [99,19,30]\n",
    "        for iplot in range(3):\n",
    "            var2plot = ds_dep_tot[var2plot_lst[iplot]].isel(step = icheance) \n",
    "            if iplot == 0 : \n",
    "                cmap  = matplotlib.cm.jet\n",
    "            else: \n",
    "                cmap = matplotlib.cm.tab20b\n",
    "                     \n",
    "            varmin   = varmin_lst[iplot]\n",
    "            varmax   = varmax_lst[iplot] + 1        \n",
    "            clevs    = np.arange(varmin,varmax+1,1)\n",
    "            cs       = var2plot.plot.imshow(ax = ax[iplot],cmap=cmap,levels=clevs)\n",
    "            for icible,cible in enumerate(zones_cibles):\n",
    "                for zone_select in  zones_cibles[cible] :\n",
    "                    mask_ref = ds_mask.sel(id = zone_select)\n",
    "\n",
    "                    list_neighbours = find_neighbours(mask_ref,listMasks)\n",
    "                    lst_mask_not_included, lst_mask_included = get_not_included_masks(mask_ref.mask, list_neighbours,ds_mask,flag_strictly_included=True)\n",
    "                    for neighbours in lst_mask_not_included:\n",
    "                        ind = np.where((mask_ref.mask.values == 1) & (ds_mask.sel(id=neighbours).mask.values == 1))\n",
    "                        ax[iplot].scatter(X[ind],Y[ind],color='k',s=6)\n",
    "                    # \n",
    "                    # ajout de la legende\n",
    "                    indice_mask_ref = np.where(mask_ref.mask.values == 1)\n",
    "\n",
    "                    ax[iplot].text(X[indice_mask_ref].mean(),Y[indice_mask_ref].mean(),s=legende[icible],color='k',fontsize=15)\n",
    "                    ax[iplot].set_title(date+' + {} h'.format(echeance))\n",
    "                    if iplot ==0:\n",
    "                        label = zone_select +': '+ legend_cible[icible] + ' ({})'.format(cible)\n",
    "                        # ajout de l'agregation: \n",
    "                        for name in list_name: \n",
    "                            val_agrege = ds_distance_dict[name].wwmf_2[ds_distance_dict[name].argmin(\"wwmf_2\")].sel(id=zone_select).isel(step=icheance).values\n",
    "                            label += ' {}:{}'.format(name,val_agrege)\n",
    "                if iplot == 0:\n",
    "                    patches.append(mlines.Line2D([],[],label = label,marker='${}$'.format(legende[icible]),color='black'))\n",
    "        lgd = ax[2].legend(handles=patches,bbox_to_anchor=(0.5,-0.2), loc='upper right',labelspacing =2,fontsize = 14)\n",
    "        fig.tight_layout()\n",
    "        fname_fig = dir_fig + 'v6_zonage_'+dep_id+date+'_'+str(echeance)+'.png'\n",
    "        print(fname_fig)\n",
    "        fig.savefig(fname_fig,dpi=400,bbox_inches='tight',format='png',bbox_extra_artists=(lgd,),)\n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        print('temps',time.time()-tdeb)\n",
    "    print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
